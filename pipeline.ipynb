{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "345e6aa4",
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install azure-ai-ml\n",
                "%pip install azureml-core\n",
                "%pip install azure-identity\n",
                "%pip install azureml-rag\n",
                "%pip install azureml.fsspec\n",
                "%pip install pandas\n",
                "%pip install openai~=0.27.8 # versioning for to allow dataplane deployment inferring"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "038912d8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# If `import win32file` fails with a DLL error then run the following and restart kernel:\n",
                "# %pip uninstall -y pywin32\n",
                "# %conda install -y --force-reinstall pywin32"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "12eb9e1f",
            "metadata": {},
            "source": [
                "# QA Data Generation\n",
                "\n",
                "QA Data Generation is a part of RAG (Retrieval Augemented Generation) creation process where the autogenerated QA dataset is used:\n",
                "\n",
                "1. To get the best prompt for RAG\n",
                "2. To get evaluation metrics for RAG\n",
                "\n",
                "This notebook shows you how to create a QA dataset from your data (Git repo). We run just the components needed for QA Data Generation and not for the full RAG creation flow."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "1a99955c",
            "metadata": {},
            "source": [
                "## Get client for AzureML Workspace\n",
                "\n",
                "The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "63178816",
            "metadata": {},
            "outputs": [],
            "source": [
                "## User Inputs\n",
                "subscription_id = \"\"\n",
                "resource_group = \"\"\n",
                "workspace_name = \"\"\n",
                "openai_connection_subscription_id = \"\"\n",
                "openai_connection_resource_group = \"\"\n",
                "openai_connection_workspace_name = \"\"\n",
                "\n",
                "# Defaults\n",
                "registry_name = \"azureml-preview\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b1af37c8",
            "metadata": {},
            "outputs": [],
            "source": [
                "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
                "from azure.ai.ml import MLClient\n",
                "from azureml.core import Workspace\n",
                "\n",
                "try:\n",
                "    credential = DefaultAzureCredential()\n",
                "    # Check if given credential can get token successfully.\n",
                "    credential.get_token(\"https://management.azure.com/.default\")\n",
                "except Exception as ex:\n",
                "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential does not work\n",
                "    credential = InteractiveBrowserCredential()\n",
                "\n",
                "try:\n",
                "    ml_client = MLClient.from_config(credential=credential, path='workspace.json')\n",
                "except Exception as ex:    \n",
                "    ml_client = MLClient(\n",
                "        credential=credential,\n",
                "        subscription_id=subscription_id,\n",
                "        resource_group_name=resource_group,\n",
                "        workspace_name=workspace_name\n",
                "    )\n",
                "ws = Workspace(subscription_id=ml_client.subscription_id, resource_group=ml_client.resource_group_name, workspace_name=ml_client.workspace_name)\n",
                "openai_ws = Workspace(subscription_id=openai_connection_subscription_id, resource_group=openai_connection_resource_group, workspace_name=openai_connection_workspace_name)\n",
                "print(ml_client)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "30906d39",
            "metadata": {},
            "source": [
                "## Azure OpenAI\n",
                "\n",
                "We recommend using gpt-35-turbo model to get good quality QAs. [Follow these instructions](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal) to setup an Azure OpenAI Instance and deploy the model. Once you have the model deployed in AOAI you can specify your Model name and Deployment name below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ab3f1c33",
            "metadata": {},
            "outputs": [],
            "source": [
                "aoai_completion_model_name = 'gpt-35-turbo'\n",
                "aoai_completion_deployment_name = 'gpt35turbo'\n",
                "aoai_embedding_model_name = 'text-embedding-ada-002'\n",
                "aoai_embedding_deployment_name = 'test'\n",
                "aoai_connection = \"christest123\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "129ac0b7",
            "metadata": {},
            "outputs": [],
            "source": [
                "from azureml.rag.utils.connections import get_connection_by_name_v2, create_connection_v2\n",
                "\n",
                "try:\n",
                "    aoai_connection = get_connection_by_name_v2(openai_ws, aoai_connection)\n",
                "    aoai_connection_id = aoai_connection['id']\n",
                "except Exception as ex:\n",
                "    print(f\"Could not get connection '{aoai_connection}', creating a new one\")\n",
                "\n",
                "    target = '<target>' # example: 'https://<endpoint>.openai.azure.com/'\n",
                "    key = '<key>'\n",
                "    apiVersion = '<api_version>' # 2023-03-15-preview\n",
                "    \n",
                "    if(key == '<key>'):\n",
                "        raise RuntimeError(f\"Please provide a valid key for the Azure OpenAI service\")\n",
                "    if(target == '<target>'):  \n",
                "        raise RuntimeError(f\"Please provide a valid target for the Azure OpenAI service\")\n",
                "    if(apiVersion == '<api_version>'):\n",
                "        raise RuntimeError(f\"Please provide a valid api-version for the Azure OpenAI service\")\n",
                "    aoai_connection_id = create_connection_v2(\n",
                "        workspace=openai_ws,\n",
                "        name=aoai_connection,\n",
                "        category='AzureOpenAI',\n",
                "        target=target,\n",
                "        auth_type='ApiKey',\n",
                "        credentials={\n",
                "            'key': key\n",
                "        },\n",
                "        metadata={\n",
                "            'apiType': 'azure',\n",
                "            'apiVersion': apiVersion\n",
                "        }\n",
                "    )['id']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c1e5cc13",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment to upgrade azureml-rag if infer_deployment is unrecognized in the package\n",
                "# %pip install azureml-rag --upgrade\n",
                "\n",
                "from azureml.rag.utils.deployment import infer_deployment\n",
                "\n",
                "aoai_completion_deployment_name = infer_deployment(aoai_connection, aoai_completion_model_name)\n",
                "print(f\"Deployment name in AOAI workspace for model '{aoai_completion_model_name}' is '{aoai_completion_deployment_name}'\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "56878876",
            "metadata": {},
            "source": [
                "### Setup Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e2a3752a",
            "metadata": {},
            "outputs": [],
            "source": [
                "ml_registry = MLClient(credential=credential, registry_name = registry_name)\n",
                "\n",
                "# validate_deployments_component = ml_registry.components.get('llm_rag_validate_deployments', label='latest')\n",
                "# git_clone_component = ml_registry.components.get('llm_rag_git_clone', label='latest')\n",
                "# crack_and_chunk_component = ml_registry.components.get('llm_rag_crack_and_chunk', label='latest')\n",
                "# data_generation_component = ml_registry.components.get('llm_rag_qa_data_generation', label='latest')\n",
                "git_to_faiss_component = ml_registry.components.get('llm_ingest_git_to_faiss_basic', label='latest')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "53285c16",
            "metadata": {},
            "outputs": [],
            "source": [
                "from azure.ai.ml import Output\n",
                "from azure.ai.ml.dsl import pipeline\n",
                "\n",
                "def use_automatic_compute(component, instance_count=1, instance_type='Standard_D2s_v3'):\n",
                "    component.set_resources(instance_count=instance_count, instance_type=instance_type, properties={'compute_specification': {'automatic': True}})\n",
                "    return component\n",
                "\n",
                "def use_aoai_connection(component, aoai_connection_id, custom_env:str=None):\n",
                "    if custom_env is not None:\n",
                "        component.environment_variables[custom_env] = aoai_connection_id  \n",
                "    if aoai_connection_id is not None:\n",
                "        component.environment_variables['AZUREML_WORKSPACE_CONNECTION_ID_AOAI'] = aoai_connection_id\n",
                "\n",
                "# @pipeline(compute=dedicated_cpu_compute)\n",
                "@pipeline(default_compute='serverless')\n",
                "def qa_faiss_index_generation(\n",
                "    git_url,\n",
                "    data_source_url,\n",
                "    llm_completion_config,\n",
                "    embeddings_model,\n",
                "    aoai_connection_id=None,\n",
                "    chunk_size=1024,\n",
                "    chunk_overlap=0,\n",
                "    chunk_prepend_summary=False,\n",
                "):\n",
                "    # validate_deployments = validate_deployments_component(\n",
                "    #     llm_config = llm_completion_config,\n",
                "    #     check_completion = \"True\",\n",
                "    #     check_embeddings = \"False\"\n",
                "    # )\n",
                "    # use_automatic_compute(validate_deployments)\n",
                "    # use_aoai_connection(validate_deployments, aoai_connection_id, custom_env='AZUREML_WORKSPACE_CONNECTION_ID_AOAI_COMPLETION')\n",
                "\n",
                "    # git_clone = git_clone_component(\n",
                "    #     git_repository=git_url,\n",
                "    #     branch_name=branch_name\n",
                "    # )\n",
                "    # use_automatic_compute(git_clone)    \n",
                "\n",
                "    # crack_and_chunk = crack_and_chunk_component(\n",
                "    #     input_data=git_clone.outputs.output_data,\n",
                "    #     input_glob=data_source_glob,\n",
                "    #     chunk_size=1024,\n",
                "    #     data_source_url=data_source_url,\n",
                "    #     document_path_replacement_regex=document_path_replacement_regex\n",
                "    # )\n",
                "    # use_automatic_compute(crack_and_chunk)\n",
                "\n",
                "    #  # QA Data Generation\n",
                "    # data_generation = data_generation_component(\n",
                "    #     input_data = crack_and_chunk.outputs.output_chunks,\n",
                "    #     deployment_validation = validate_deployments.outputs.output_data,\n",
                "    #     llm_config = llm_completion_config,\n",
                "    #     dataset_size = 10,  # Number of QAs to be generated\n",
                "    #     dataset_name = asset_name,\n",
                "    #     register_output =  True\n",
                "    # )\n",
                "    # use_automatic_compute(data_generation)\n",
                "    # use_aoai_connection(data_generation, aoai_connection_id)\n",
                "\n",
                "    # Ingest Git to Faiss Vector Index\n",
                "    git_to_faiss = git_to_faiss_component(\n",
                "        git_repository = git_url,\n",
                "        data_source_url = data_source_url,\n",
                "        llm_config = llm_completion_config,\n",
                "        llm_connection = aoai_connection_id,\n",
                "        embeddings_model = embeddings_model,\n",
                "        embedding_connection = aoai_connection_id,\n",
                "        chunk_size = chunk_size,\n",
                "        chunk_overlap = chunk_overlap,\n",
                "        chunk_prepend_summary = chunk_prepend_summary\n",
                "    )\n",
                "    use_automatic_compute(git_to_faiss)\n",
                "\n",
                "\n",
                "    return {\n",
                "        'qa_faiss_index': git_to_faiss.outputs.faiss_index,\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2cd4b8bf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# User Inputs\n",
                "git_url = 'https://github.com/microsoft/ml-wrappers'\n",
                "data_source_url = 'https://github.com/microsoft/ml-wrappers'\n",
                "chunk_size = \"1024\"\n",
                "chunk_overlap = \"0\"\n",
                "chunk_prepend_summary = False\n",
                "#data_source_glob = 'articles/machine-learning/**/*'\n",
                "\n",
                "## This regex is used to remove the 'articles' folder from the source url put in each files metadata in the index.\n",
                "#document_path_replacement_regex = r'{\"match_pattern\": \"(.*)/articles/(.*)(\\\\.[^.]+)$\", \"replacement_pattern\": \"\\\\1/\\\\2\"}'\n",
                "#asset_name = 'qa_data'\n",
                "experiment_name = 'qa_faiss_index_generation'\n",
                "\n",
                "# Defaults\n",
                "embeddings_model = f'{{azure_open_ai://deployment/{aoai_embedding_deployment_name}/model/{aoai_embedding_model_name}}}'\n",
                "llm_completion_config = f'{{\"type\":\"azure_open_ai\",\"model_name\":\"{aoai_completion_model_name}\",\"deployment_name\":\"{aoai_completion_deployment_name}\",\"temperature\":\"0\",\"max_tokens\":\"2000\"}}'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f5d18c8",
            "metadata": {},
            "outputs": [],
            "source": [
                "from azure.ai.ml import Input\n",
                "from azure.ai.ml.entities import UserIdentityConfiguration\n",
                "\n",
                "# data_source_glob=data_source_glob,\n",
                "# asset_name=asset_name,\n",
                "# document_path_replacement_regex=document_path_replacement_regex,\n",
                "pipeline_job = qa_faiss_index_generation(\n",
                "    git_url = git_url,\n",
                "    data_source_url = data_source_url,\n",
                "    llm_completion_config = llm_completion_config,\n",
                "    embeddings_model = embeddings_model,\n",
                "    aoai_connection_id=aoai_connection_id,\n",
                "    chunk_size = chunk_size,\n",
                "    chunk_overlap = chunk_overlap,\n",
                "    chunk_prepend_summary = chunk_prepend_summary,\n",
                ")\n",
                "\n",
                "pipeline_job.identity = UserIdentityConfiguration()\n",
                "pipeline_job.settings.continue_on_step_failure = False\n",
                "\n",
                "# pipeline_job.settings.force_rerun = True # Rerun each time so that git_clone isn't cached, if intent is to ingest latest data."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "67f89a25",
            "metadata": {},
            "source": [
                "### Submit Pipeline\n",
                "Click on the generated link below access the job details on studio. Make sure all necessary flights are added on the URL to access these preview features.\n",
                "\n",
                "**In case of any errors see [TROUBLESHOOT.md](../../TROUBLESHOOT.md).**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "52d9b2be",
            "metadata": {},
            "outputs": [],
            "source": [
                "running_pipeline_job = ml_client.jobs.create_or_update(\n",
                "    pipeline_job, experiment_name=experiment_name\n",
                ")\n",
                "running_pipeline_job"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "ed144c98",
            "metadata": {},
            "source": [
                "### Review generated QA data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "174bd7de",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import fsspec\n",
                "# import pandas as pd\n",
                "\n",
                "# qa_data = ml_client.data.get(f\"{asset_name}-test-data\", label='latest')\n",
                "# with fsspec.open(qa_data.path) as f:\n",
                "#     df = pd.read_json(f, lines=True)\n",
                "# df.head(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d236cf6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# for qa_type in [\"TOPIC\", \"FACTUAL\", \"BOOLEAN\"]:\n",
                "#     print(f\"{qa_type} Question Answers:\")\n",
                "#     for _, row in df[df[\"qaType\"] == qa_type][:2].iterrows():\n",
                "#         print(\"Q:\", row[\"question\"])\n",
                "#         print(\"A:\", row[\"answer\"])\n",
                "#         print()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "e598a8a4",
            "metadata": {},
            "source": [
                "### Review token usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f97a1a98",
            "metadata": {},
            "outputs": [],
            "source": [
                "# running_pipeline_job = ml_client.jobs.get(\"<pipeline run id>\")\n",
                "child_runs = ml_client.jobs.list(parent_job_name=running_pipeline_job.name)\n",
                "child_runs = list(child_runs)\n",
                "data_generation_run = child_runs[-1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "369793ed",
            "metadata": {},
            "outputs": [],
            "source": [
                "from azureml.core import Run\n",
                "\n",
                "run = Run.get(ws, data_generation_run.name)\n",
                "metrics = run.get_metrics()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dbed9c85",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Tokens used: {metrics['total_tokens']}\")\n",
                "print(f\"Model used: {metrics['llm_model_name']}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "69b9772c",
            "metadata": {},
            "source": [
                "Given the token usage and the model you can compute cost using the pricing here: https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
